# BPE è®­ç»ƒé€Ÿåº¦ä¼˜åŒ–æŒ‡å—

## ğŸš€ æ ¸å¿ƒé—®é¢˜åˆ†æ

å½“å‰çš„å¹¶è¡ŒåŒ–å®ç°å­˜åœ¨ä»¥ä¸‹ä¸»è¦é—®é¢˜ï¼š

### 1. **é‡å¤åˆ›å»ºè¿›ç¨‹æ± å¼€é”€å·¨å¤§**
```python
# é—®é¢˜ä»£ç  (æ¯æ¬¡è¿­ä»£éƒ½åˆ›å»ºæ–°è¿›ç¨‹æ± )
for i in range(num_merges):
    with ProcessPoolExecutor(max_workers=mp.cpu_count()) as executor:
        results = executor.map(self.process_chunk_stats, chunk_data)
```

**é—®é¢˜ï¼š**
- æ¯æ¬¡åˆå¹¶è¿­ä»£éƒ½è¦åˆ›å»º/é”€æ¯è¿›ç¨‹æ± 
- è¿›ç¨‹å¯åŠ¨å¼€é”€å¯èƒ½æ¯”å®é™…è®¡ç®—æ—¶é—´è¿˜é•¿
- å¯¹äº30k+æ¬¡åˆå¹¶ï¼Œè¿™ä¸ªå¼€é”€æ˜¯å·¨å¤§çš„

### 2. **æ²¡æœ‰åˆ©ç”¨æ‰¹å¤„ç†**
- æ¯ä¸ªè¿›ç¨‹åªå¤„ç†ä¸€ä¸ªchunk
- è¿›ç¨‹é—´é€šä¿¡å¼€é”€è¿‡å¤§
- æ— æ³•å……åˆ†åˆ©ç”¨CPU cache

### 3. **æ²¡æœ‰è¿‡æ»¤ä½é¢‘æ•°æ®**
- å¤„ç†æ‰€æœ‰chunkï¼ŒåŒ…æ‹¬åªå‡ºç°1æ¬¡çš„
- æµªè´¹è®¡ç®—èµ„æºåœ¨å¯¹ç»“æœå½±å“å¾ˆå°çš„æ•°æ®ä¸Š

## ğŸ’¡ æ”¹è¿›æ–¹æ¡ˆ

### **æ–¹æ¡ˆ1: è¿›ç¨‹æ± å¤ç”¨ (ç«‹å³å¯ç”¨)**

åªéœ€ä¿®æ”¹ç°æœ‰çš„å¹¶è¡ŒåŒ–ä»£ç ï¼š

```python
class OptimizedRegexTokenizer(Tokenizer):
    def __init__(self, max_workers=None):
        super().__init__()
        # ... å…¶ä»–åˆå§‹åŒ–ä»£ç  ...
        self.max_workers = max_workers or min(mp.cpu_count(), 8)
        self.executor = None
    
    def _get_executor(self):
        if self.executor is None:
            self.executor = ProcessPoolExecutor(max_workers=self.max_workers)
        return self.executor
    
    def train(self, text, vocab_size, verbose=False):
        # ... å‰é¢çš„ä»£ç  ...
        
        try:
            executor = self._get_executor()  # å¤ç”¨åŒä¸€ä¸ªè¿›ç¨‹æ± 
            
            for i in range(num_merges):
                chunk_data = [(chunk_ids, text_chunks[j], word_freq.get(text_chunks[j], 1)) 
                             for j, chunk_ids in enumerate(ids)]
                
                # ä½¿ç”¨å·²å­˜åœ¨çš„è¿›ç¨‹æ± 
                results = executor.map(self.process_chunk_stats, chunk_data)
                
                # ... åˆå¹¶é€»è¾‘ ...
        finally:
            if self.executor:
                self.executor.shutdown(wait=True)
```

**é¢„æœŸæ•ˆæœï¼š** 2-3å€é€Ÿåº¦æå‡

### **æ–¹æ¡ˆ2: æ‰¹å¤„ç†ä¼˜åŒ– (ä¸­ç­‰éš¾åº¦)**

```python
def process_chunk_batch(chunk_batch_data):
    """ä¸€æ¬¡å¤„ç†å¤šä¸ªchunkï¼Œå‡å°‘è¿›ç¨‹é—´é€šä¿¡å¼€é”€"""
    chunk_batch, word_freq_dict = chunk_batch_data
    stats = defaultdict(int)
    
    for chunk_ids, chunk in chunk_batch:
        chunk_weight = word_freq_dict.get(chunk, 1)
        # å†…è”get_statsé€»è¾‘ä»¥æé«˜æ€§èƒ½
        for i in range(len(chunk_ids) - 1):
            pair = (chunk_ids[i], chunk_ids[i + 1])
            stats[pair] += chunk_weight
    
    return dict(stats)

def _process_with_batching(self, ids, text_chunks, word_freq, batch_size=1000):
    executor = self._get_executor()
    
    # åˆ›å»ºæ‰¹æ¬¡
    batches = []
    word_freq_dict = dict(word_freq)
    
    for i in range(0, len(ids), batch_size):
        batch_end = min(i + batch_size, len(ids))
        chunk_batch = [(ids[j], text_chunks[j]) for j in range(i, batch_end)]
        batches.append((chunk_batch, word_freq_dict))
    
    # å¹¶è¡Œå¤„ç†æ‰¹æ¬¡
    futures = [executor.submit(process_chunk_batch, batch_data) 
              for batch_data in batches]
    
    # æ”¶é›†ç»“æœ
    stats = defaultdict(int)
    for future in as_completed(futures):
        batch_stats = future.result()
        for pair, count in batch_stats.items():
            stats[pair] += count
    
    return dict(stats)
```

**é¢„æœŸæ•ˆæœï¼š** åœ¨æ–¹æ¡ˆ1åŸºç¡€ä¸Šå†æå‡1.5-2å€

### **æ–¹æ¡ˆ3: é¢‘ç‡è¿‡æ»¤ (ç®€å•æœ‰æ•ˆ)**

```python
def _filter_low_frequency_chunks(self, text_chunks, word_freq, min_threshold=2):
    """è¿‡æ»¤ä½é¢‘chunkï¼Œå‡å°‘è®¡ç®—é‡"""
    filtered_chunks = [chunk for chunk in text_chunks 
                      if word_freq[chunk] >= min_threshold]
    filtered_freq = {chunk: word_freq[chunk] for chunk in filtered_chunks}
    return filtered_chunks, filtered_freq

# åœ¨trainæ–¹æ³•ä¸­ä½¿ç”¨
text_chunks, word_freq = self._filter_low_frequency_chunks(
    list(word_freq.keys()), word_freq, min_threshold=2)
```

**é¢„æœŸæ•ˆæœï¼š** å‡å°‘20-40%çš„è®¡ç®—é‡

### **æ–¹æ¡ˆ4: å†…å­˜ä¼˜åŒ–**

```python
# ä½¿ç”¨æ›´ç´§å‡‘çš„æ•°æ®ç»“æ„
from collections import defaultdict

# é¿å…é‡å¤çš„å­—å…¸æŸ¥æ‰¾
word_freq_dict = dict(word_freq)  # è½¬æ¢ä¸€æ¬¡ï¼Œé‡å¤ä½¿ç”¨

# ä½¿ç”¨ç”Ÿæˆå™¨å‡å°‘å†…å­˜å ç”¨
def chunk_generator(ids, text_chunks, word_freq_dict):
    for j, chunk_ids in enumerate(ids):
        yield (chunk_ids, text_chunks[j], word_freq_dict.get(text_chunks[j], 1))
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”é¢„æœŸ

| å®ç°æ–¹å¼ | ç›¸å¯¹åŸç‰ˆé€Ÿåº¦ | å†…å­˜ä½¿ç”¨ | å®ç°éš¾åº¦ |
|----------|-------------|----------|----------|
| åŸç‰ˆä¸²è¡Œ | 1x (åŸºå‡†) | ä½ | - |
| å½“å‰å¹¶è¡Œ | 1.5-2x | ä¸­ç­‰ | - |
| è¿›ç¨‹æ± å¤ç”¨ | 3-4x | ä¸­ç­‰ | ç®€å• |
| + æ‰¹å¤„ç† | 5-8x | ä¸­ç­‰ | ä¸­ç­‰ |
| + é¢‘ç‡è¿‡æ»¤ | 6-10x | ä½ | ç®€å• |
| å…¨éƒ¨ä¼˜åŒ– | 8-15x | ä½ | ä¸­ç­‰ |

## ğŸ› ï¸ å®æ–½å»ºè®®

### **é˜¶æ®µ1: å¿«é€Ÿæ”¹è¿› (1å°æ—¶å†…å®Œæˆ)**
1. å®ç°è¿›ç¨‹æ± å¤ç”¨
2. æ·»åŠ é¢‘ç‡è¿‡æ»¤
3. æµ‹è¯•æ€§èƒ½æå‡

### **é˜¶æ®µ2: æ·±åº¦ä¼˜åŒ– (åŠå¤©)**
1. å®ç°æ‰¹å¤„ç†
2. ä¼˜åŒ–å†…å­˜ä½¿ç”¨
3. æ·»åŠ è¿›åº¦æ˜¾ç¤ºå’ŒETA

### **é˜¶æ®µ3: é«˜çº§ä¼˜åŒ– (å¯é€‰)**
1. è€ƒè™‘ä½¿ç”¨Cython/NumbaåŠ é€Ÿæ ¸å¿ƒå‡½æ•°
2. å®ç°å¢é‡ç»Ÿè®¡æ›´æ–°
3. GPUåŠ é€Ÿï¼ˆå¦‚æœæ•°æ®é‡å¾ˆå¤§ï¼‰

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

```python
# åŸºç¡€ä¼˜åŒ–ä½¿ç”¨
from minbpe.regex_optimized import OptimizedRegexTokenizer

# å¯¹äºä¸­ç­‰æ•°æ®é›†
tokenizer = OptimizedRegexTokenizer(
    max_workers=4,
    batch_size=1000,
    min_freq_threshold=2
)

# å¯¹äºå¤§æ•°æ®é›†
tokenizer = OptimizedRegexTokenizer(
    max_workers=8,
    batch_size=2000,
    min_freq_threshold=5,
    use_batching=True
)

# ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿èµ„æºæ¸…ç†
with tokenizer:
    tokenizer.train(text, vocab_size=32000, verbose=True)
    tokenizer.save("model")
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **CPUæ ¸å¿ƒæ•°é™åˆ¶ï¼š** ä¸è¦è®¾ç½®è¶…è¿‡8ä¸ªworkerï¼Œé€šå¸¸4-6ä¸ªæœ€ä¼˜
2. **å†…å­˜ç›‘æ§ï¼š** å¤§batch_sizeå¯èƒ½å¢åŠ å†…å­˜ä½¿ç”¨
3. **é¢‘ç‡é˜ˆå€¼ï¼š** min_freq_threshold=2é€šå¸¸æ˜¯å¥½çš„èµ·ç‚¹
4. **æ•°æ®é›†å¤§å°ï¼š** å°æ•°æ®é›†å¯èƒ½ä¸éœ€è¦æ‰€æœ‰ä¼˜åŒ–

## ğŸ” è°ƒè¯•å’Œç›‘æ§

```python
# æ·»åŠ è¯¦ç»†çš„æ€§èƒ½ç›‘æ§
def train_with_monitoring(self, text, vocab_size, verbose=True):
    import psutil
    import os
    
    process = psutil.Process(os.getpid())
    
    print(f"Initial memory usage: {process.memory_info().rss / 1024 / 1024:.1f} MB")
    print(f"CPU cores available: {mp.cpu_count()}")
    print(f"Workers configured: {self.max_workers}")
    
    # ... è®­ç»ƒé€»è¾‘ ...
    
    if verbose and i % 100 == 0:
        mem_usage = process.memory_info().rss / 1024 / 1024
        cpu_percent = process.cpu_percent()
        print(f"Iteration {i}: Memory={mem_usage:.1f}MB, CPU={cpu_percent:.1f}%")
```

è¿™äº›ä¼˜åŒ–æ–¹æ¡ˆå¯ä»¥æ ¹æ®ä½ çš„å…·ä½“éœ€æ±‚å’Œæ•°æ®é›†å¤§å°é€æ­¥å®æ–½ã€‚å»ºè®®å…ˆä»æœ€ç®€å•çš„è¿›ç¨‹æ± å¤ç”¨å¼€å§‹ï¼ 