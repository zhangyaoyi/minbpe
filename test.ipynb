{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minbpe.regex import RegexTokenizer\n",
    "\n",
    "tokenizer = RegexTokenizer()\n",
    "\n",
    "tokenizer.load(\"models/regex.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[117, 874, 501, 509, 265, 324, 620, 372, 262, 923, 474, 44, 340, 1986, 2620, 350, 46, 284, 3184, 544, 407, 900, 330, 262, 398, 486, 46, 341, 282, 386, 810, 266, 262, 3184, 1093, 560, 265, 2580, 337, 46, 318, 4399, 4261, 337, 266, 399, 1110, 483, 900, 46, 284, 3184, 366, 515, 344, 703, 358, 46, 100257]\n",
      "u don't have to be scared of the loud dog, I'll protect you. The mole felt so safe with the little girl. She was very kind and the mole soon came to trust her. He leaned against her and she kept him safe. The mole had found his best friend.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# 规范的测试数据\n",
    "encoded_text = tokenizer.encode(\"u don't have to be scared of the loud dog, I'll protect you. The mole felt so safe with the little girl. She was very kind and the mole soon came to trust her. He leaned against her and she kept him safe. The mole had found his best friend.<|endoftext|>\", allowed_special=\"all\")\n",
    "\n",
    "print(encoded_text)\n",
    "\n",
    "print(tokenizer.decode(encoded_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' by'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([867])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 规范的长文本数据\n",
    "encoded_text = tokenizer.encode(\"\"\"Once upon a time there was a little girl named Lucy. She loved to go to the store to buy sweets with her mom and dad. On this special day, Lucy entered the store with her mom and dad, feeling so excited.\n",
    "As they were looking around, Lucy noticed a little girl playing with a toy in the corner of the store. She gasped in excitement and ran towards her. Lucy asked if she could play too but the little girl said no. She was rather grumpy and was not in the mood to play.\n",
    "Lucy's mom saw what was going on and told Lucy, \"Let's try to be peaceful and kind to her. Have patience and understanding. Together, you can both be happy!\"\n",
    "So, Lucy smiled at the girl and said, \"Can we play together?\" The little girl softened and smiled back. She agreed to share the toy and even let Lucy have a turn first.\n",
    "Lucy and the little girl played together happily. In the end, they both learnt an important lesson: be peaceful, kind, and understanding when faced with a conflict. And that is why Lucy and the little girl became great friends.\n",
    "<|endoftext|>\"\"\", allowed_special=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[438, 447, 258, 404, 408, 282, 258, 398, 486, 409, 697, 46, 341, 512, 265, 489, 265, 262, 1119, 265, 1446, 6880, 330, 337, 394, 266, 735, 46, 2251, 741, 862, 329, 44, 697, 4921, 262, 1119, 330, 337, 394, 266, 735, 44, 1342, 407, 742, 292, 932, 363, 412, 1150, 635, 44, 697, 1562, 258, 398, 486, 595, 330, 258, 453, 315, 262, 2770, 372, 262, 1119, 46, 341, 4224, 315, 4052, 266, 605, 2098, 337, 46, 697, 551, 851, 399, 475, 327, 430, 416, 262, 398, 486, 326, 554, 46, 341, 282, 7429, 2240, 266, 282, 367, 315, 262, 8187, 265, 327, 292, 1598, 392, 394, 390, 588, 282, 1125, 354, 266, 751, 697, 44, 319, 910, 392, 826, 265, 324, 2679, 266, 810, 265, 337, 46, 5016, 6175, 266, 5387, 46, 2545, 44, 350, 476, 732, 324, 384, 678, 1397, 44, 697, 572, 457, 262, 486, 266, 326, 44, 319, 1042, 365, 327, 466, 485, 284, 398, 486, 9591, 266, 572, 522, 46, 341, 1272, 265, 850, 262, 453, 266, 885, 843, 697, 509, 258, 897, 1189, 292, 1598, 266, 262, 398, 486, 482, 466, 1000, 46, 972, 262, 882, 44, 363, 732, 4963, 428, 1219, 1531, 58, 324, 2679, 44, 810, 44, 266, 5387, 641, 6402, 330, 258, 6417, 46, 730, 389, 439, 1490, 697, 266, 262, 398, 486, 702, 973, 419, 292, 100257]\n",
      "Once upon a time there was a little girl named Lucy. She loved to go to the store to buy sweets with her mom and dad. On this special day, Lucy entered the store with her mom and dad, feeling so excited.\n",
      "As they were looking around, Lucy noticed a little girl playing with a toy in the corner of the store. She gasped in excitement and ran towards her. Lucy asked if she could play too but the little girl said no. She was rather grumpy and was not in the mood to play.\n",
      "Lucy's mom saw what was going on and told Lucy, \"Let's try to be peaceful and kind to her. Have patience and understanding. Together, you can both be happy!\"\n",
      "So, Lucy smiled at the girl and said, \"Can we play together?\" The little girl softened and smiled back. She agreed to share the toy and even let Lucy have a turn first.\n",
      "Lucy and the little girl played together happily. In the end, they both learnt an important lesson: be peaceful, kind, and understanding when faced with a conflict. And that is why Lucy and the little girl became great friends.\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(encoded_text)\n",
    "print(tokenizer.decode(encoded_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[647, 103, 104, 98, 647, 102, 100, 272, 1417, 115, 100, 102, 106, 868, 115, 32]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'asghbasfdreafsdfjlfs '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 不存在的单词\n",
    "encoded_text = tokenizer.encode(\"asghbasfdreafsdfjlfs \")\n",
    "print(encoded_text)\n",
    "tokenizer.decode(encoded_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ishghb'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([642, 103, 104, 98])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[228, 189, 160, 229, 165, 189, 239, 188, 140, 228, 187, 138, 229, 164, 169, 229, 164, 169, 230, 176, 148, 230, 128, 142, 228, 185, 136, 230, 160, 183, 239, 188, 159]\n",
      "你好，今天天气怎么样？\n"
     ]
    }
   ],
   "source": [
    "# 没有经过训练的语言\n",
    "encoded_text = tokenizer.encode(\"你好，今天天气怎么样？\")\n",
    "print(encoded_text)\n",
    "print(tokenizer.decode(encoded_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([228, 189, 160])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[230, 151, 165, 230, 156, 172, 232, 170, 158, 227, 129, 174, 227, 131, 134, 227, 130, 173, 227, 130, 185, 227, 131, 136, 227, 129, 175, 227, 128, 129, 228, 184, 187, 227, 129, 171, 51, 227, 129, 164, 227, 129, 174, 232, 161, 168, 232, 168, 152, 228, 189, 147, 231, 179, 187, 227, 129, 167, 230, 155, 184, 227, 129, 139, 227, 130, 140, 227, 129, 166, 227, 129, 132, 227, 129, 190, 227, 129, 153, 227, 128, 130, 230, 188, 162, 229, 173, 151, 239, 188, 136, 228, 184, 173, 229, 155, 189, 232, 170, 158, 231, 148, 177, 230, 157, 165, 227, 129, 174, 232, 161, 168, 230, 132, 143, 230, 150, 135, 229, 173, 151, 239, 188, 137, 227, 128, 129, 227, 129, 178, 227, 130, 137, 227, 129, 140, 227, 129, 170, 227, 128, 129, 227, 130, 171, 227, 130, 191, 227, 130, 171, 227, 131, 138, 239, 188, 136, 50, 227, 129, 164, 227, 129, 174, 233, 159, 179, 231, 175, 128, 230, 150, 135, 229, 173, 151, 227, 129, 167, 227, 128, 129, 49, 227, 129, 164, 227, 129, 175, 230, 151, 165, 230, 156, 172, 232, 170, 158, 227, 129, 174, 229, 141, 152, 232, 170, 158, 227, 130, 146, 227, 128, 129, 227, 130, 130, 227, 129, 134, 49, 227, 129, 164, 227, 129, 175, 229, 164, 150, 230, 157, 165, 232, 170, 158, 227, 130, 146, 232, 161, 168, 232, 168, 152, 227, 129, 153, 227, 130, 139, 227, 129, 159, 227, 130, 129, 227, 129, 174, 227, 130, 130, 227, 129, 174, 227, 129, 167, 227, 129, 153, 239, 188, 137, 227, 129, 167, 227, 129, 153, 227, 128, 130, 227, 129, 149, 227, 130, 137, 227, 129, 171, 227, 128, 129, 227, 130, 162, 227, 131, 169, 227, 131, 147, 227, 130, 162, 230, 149, 176, 229, 173, 151, 227, 129, 168, 227, 131, 173, 227, 131, 188, 227, 131, 158, 229, 173, 151, 227, 130, 130, 228, 189, 191, 231, 148, 168, 227, 129, 149, 227, 130, 140, 227, 129, 190, 227, 129, 153, 227, 128, 130]\n",
      "日本語のテキストは、主に3つの表記体系で書かれています。漢字（中国語由来の表意文字）、ひらがな、カタカナ（2つの音節文字で、1つは日本語の単語を、もう1つは外来語を表記するためのものです）です。さらに、アラビア数字とローマ字も使用されます。\n"
     ]
    }
   ],
   "source": [
    "# 没有经过训练的日语\n",
    "encoded_text = tokenizer.encode(\"日本語のテキストは、主に3つの表記体系で書かれています。漢字（中国語由来の表意文字）、ひらがな、カタカナ（2つの音節文字で、1つは日本語の単語を、もう1つは外来語を表記するためのものです）です。さらに、アラビア数字とローマ字も使用されます。\")\n",
    "print(encoded_text)\n",
    "print(tokenizer.decode(encoded_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'日'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([230,151,165])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79, 5605, 111, 459, 665, 290, 195, 170, 115, 32, 195, 169, 5480, 114, 3708, 1643, 758, 195, 170, 115, 259, 662, 974, 647, 1346, 99, 946, 6475, 1066, 5480, 114, 7676, 58, 957, 302, 106, 105, 32, 40, 99, 778, 971, 1859, 115, 825, 767, 114, 195, 161, 2892, 1666, 276, 281, 3235, 6728, 382, 9420, 195, 170, 115, 41, 44, 310, 306, 552, 6043, 383, 957, 297, 894, 6043, 32, 40, 100, 117, 647, 5480, 114, 273, 647, 3241, 632, 195, 161, 322, 647, 44, 325, 1116, 278, 778, 5480, 272, 321, 4110, 1392, 458, 115, 459, 665, 2490, 647, 288, 297, 3235, 647, 383, 493, 458, 278, 778, 5480, 272, 321, 4110, 1392, 458, 115, 383, 9628, 1152, 306, 647, 41, 46, 3055, 195, 169, 109, 1247, 1069, 44, 562, 9388, 286, 109, 1666, 1440, 195, 161, 98, 472, 1666, 383, 843, 458, 115, 336, 2534, 647, 259, 195, 163, 111, 834, 6728, 4658, 139, 710, 139, 6960, 5605, 111, 459, 665, 290, 195, 170, 115, 46]\n",
      "O texto japonês é escrito em três sistemas principais de escrita: Kanji (caracteres ideográficos derivados do chinês), Hiragana e Katakana (duas escritas silabárias, uma para escrever palavras japonesas nativas e outra para escrever palavras estrangeiras). Além disso, algarismos arábicos e letras romanas são usados ​​no texto japonês.\n"
     ]
    }
   ],
   "source": [
    "encoded_text = tokenizer.encode(\"\"\"O texto japonês é escrito em três sistemas principais de escrita: Kanji (caracteres ideográficos derivados do chinês), Hiragana e Katakana (duas escritas silabárias, uma para escrever palavras japonesas nativas e outra para escrever palavras estrangeiras). Além disso, algarismos arábicos e letras romanas são usados ​​no texto japonês.\"\"\")\n",
    "print(encoded_text)\n",
    "print(tokenizer.decode(encoded_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "�\n"
     ]
    }
   ],
   "source": [
    "tokenizer.decode([79, 5529, 111, 457, 660, 290, 195, 170])\n",
    "print(tokenizer.decode([170]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是因为RegexTokenizer使用了字节级别的编码方式。当tokenizer遇到未训练过的语言（如日语、中文）时，它会将这些字符拆分成UTF-8字节序列，然后对每个字节进行单独编码。\n",
    "从您的测试可以看到:\n",
    "当您编码\"你好\"时，它被拆分成了[228, 189, 160, 229, 165, 189]这样的字节序列\n",
    "当您解码单个token [228, 189, 160]时，正好得到了\"你\"这个字符\n",
    "这种方法的好处是可以处理任何UTF-8编码的文本，即使tokenizer没有专门针对该语言进行训练。缺点是对于非英语文本，编码效率较低，因为每个非ASCII字符会被拆分成多个token（通常是3个字节）。\n",
    "这种字节级编码是许多现代tokenizer（如GPT系列使用的tiktoken）的基础机制，确保了模型可以处理任何语言的输入，即使是训练数据中未见过的语言。\n",
    "\n",
    "是的，您的理解完全正确。使用英文文本训练的tokenizer在处理全中文文本时，编码后的token序列会非常长。\n",
    "\n",
    "这是因为：\n",
    "\n",
    "1. 英文训练的tokenizer不会有中文字符的专门token\n",
    "2. 每个中文字符在UTF-8编码中通常占3个字节\n",
    "3. 当遇到未知的中文字符时，tokenizer会回退到字节级编码\n",
    "\n",
    "所以一个只包含10个中文字符的短句，可能会被编码成30个token左右，而同样语义的英文句子可能只需要5-10个token。\n",
    "\n",
    "这种编码效率的差异会导致：\n",
    "- 处理非训练语言时token用量增加\n",
    "- 在有token限制的API中，非英文内容能处理的实际信息量减少\n",
    "- 对于收费按token计费的API，处理非英语内容的成本更高\n",
    "\n",
    "要提高对特定语言的编码效率，需要用包含该语言的语料库重新训练tokenizer。\n",
    "\n",
    "\n",
    "\n",
    "对于使用全英文文本训练的模型：\n",
    "\n",
    "1. 模型确实可以接收任何token序列作为输入，包括通过字节级编码生成的中文或日语token序列。\n",
    "\n",
    "2. 然而，模型对这些序列的\"理解\"是有限的：\n",
    "   - 模型没有见过这些特定的token模式\n",
    "   - 模型没有学习到这些token序列与语义的对应关系\n",
    "   - 模型无法理解这些语言的语法和上下文\n",
    "\n",
    "3. 模型可能会：\n",
    "   - 产生不连贯或无意义的输出\n",
    "   - 无法准确捕捉非英文内容的语义\n",
    "   - 在生成回复时出现混乱或不相关的内容\n",
    "\n",
    "所以，虽然从技术上讲tokenizer可以处理任何UTF-8文本并转换为token序列，但如果模型没有在这些语言上训练过，它就无法有效地\"理解\"或处理这些序列的语义内容。\n",
    "\n",
    "要使模型真正理解多语言内容，需要在包含这些语言的数据上进行训练或微调。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
